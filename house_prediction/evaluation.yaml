# PIPELINE DEFINITION
# Name: evaluation
# Inputs:
#    model: system.Model
#    preprocessed_dataset: system.Dataset
# Outputs:
#    evaluation-metrics: system.Metrics
#    html: system.HTML
#    metrics: system.Metrics
components:
  comp-evaluation:
    executorLabel: exec-evaluation
    inputDefinitions:
      artifacts:
        model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
        preprocessed_dataset:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
    outputDefinitions:
      artifacts:
        html:
          artifactType:
            schemaTitle: system.HTML
            schemaVersion: 0.0.1
        metrics:
          artifactType:
            schemaTitle: system.Metrics
            schemaVersion: 0.0.1
deploymentSpec:
  executors:
    exec-evaluation:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - evaluation
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.7.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef evaluation(\n    model: Input[Model],\n    preprocessed_dataset:\
          \ Input[Dataset],\n    metrics: Output[Metrics],\n    html: Output[HTML]\n\
          ):\n    \"\"\"Evaluates the model's performance and generates visualizations.\"\
          \"\"\n    import pandas as pd\n    import joblib\n    import matplotlib.pyplot\
          \ as plt\n    from sklearn.metrics import mean_squared_error, r2_score,\
          \ mean_absolute_error\n    import logging\n    import base64\n    from io\
          \ import BytesIO\n\n    logging.basicConfig(level=logging.INFO)\n\n    #\
          \ 1. Load the model and dataset\n    rf_model = joblib.load(model.path)\n\
          \    df = pd.read_csv(preprocessed_dataset.path)\n\n    logging.info(f\"\
          Loaded model and dataset with shape: {df.shape}\")\n\n    # Split features\
          \ and target\n    X = df.drop('price', axis=1)\n    y = df['price']\n\n\
          \    # 2. Make predictions\n    y_pred = rf_model.predict(X)\n\n    # 3.\
          \ Calculate metrics\n    mse = mean_squared_error(y, y_pred)\n    rmse =\
          \ mse ** 0.5\n    mae = mean_absolute_error(y, y_pred)\n    r2 = r2_score(y,\
          \ y_pred)\n\n    # 4. Save metrics\n    metrics.log_metric(\"mse\", float(mse))\n\
          \    metrics.log_metric(\"rmse\", float(rmse))\n    metrics.log_metric(\"\
          mae\", float(mae))\n    metrics.log_metric(\"r2_score\", float(r2))\n\n\
          \    logging.info(f\"MSE: {mse:.4f}\")\n    logging.info(f\"RMSE: {rmse:.4f}\"\
          )\n    logging.info(f\"MAE: {mae:.4f}\")\n    logging.info(f\"R2 Score:\
          \ {r2:.4f}\")\n\n    # 5. Create visualizations\n    fig, axes = plt.subplots(1,\
          \ 2, figsize=(14, 5))\n\n    # Actual vs Predicted\n    axes[0].scatter(y,\
          \ y_pred, alpha=0.5)\n    axes[0].plot([y.min(), y.max()], [y.min(), y.max()],\
          \ 'r--', lw=2)\n    axes[0].set_xlabel('Actual Price')\n    axes[0].set_ylabel('Predicted\
          \ Price')\n    axes[0].set_title('Actual vs Predicted Prices')\n\n    #\
          \ Feature Importance\n    feature_importance = pd.DataFrame({\n        'feature':\
          \ X.columns,\n        'importance': rf_model.feature_importances_\n    }).sort_values('importance',\
          \ ascending=True)\n\n    axes[1].barh(feature_importance['feature'], feature_importance['importance'])\n\
          \    axes[1].set_xlabel('Importance')\n    axes[1].set_title('Feature Importance')\n\
          \n    plt.tight_layout()\n\n    # Convert plot to base64\n    buf = BytesIO()\n\
          \    plt.savefig(buf, format='png', dpi=100, bbox_inches='tight')\n    buf.seek(0)\n\
          \    img_base64 = base64.b64encode(buf.read()).decode('utf-8')\n    plt.close()\n\
          \n    # 6. Create and save HTML report\n    html_content = f\"\"\"\n   \
          \ <html>\n    <head><title>Model Evaluation Report</title></head>\n    <body>\n\
          \        <h1>House Price Prediction - Model Evaluation</h1>\n        <h2>Performance\
          \ Metrics</h2>\n        <table border=\"1\" style=\"border-collapse: collapse;\
          \ padding: 10px;\">\n            <tr><th>Metric</th><th>Value</th></tr>\n\
          \            <tr><td>MSE</td><td>{mse:.4f}</td></tr>\n            <tr><td>RMSE</td><td>{rmse:.4f}</td></tr>\n\
          \            <tr><td>MAE</td><td>{mae:.4f}</td></tr>\n            <tr><td>R2\
          \ Score</td><td>{r2:.4f}</td></tr>\n        </table>\n        <h2>Visualizations</h2>\n\
          \        <img src=\"data:image/png;base64,{img_base64}\" />\n    </body>\n\
          \    </html>\n    \"\"\"\n\n    with open(html.path, 'w') as f:\n      \
          \  f.write(html_content)\n\n    logging.info(f\"Evaluation report saved\
          \ to: {html.path}\")\n\n"
        image: europe-west4-docker.pkg.dev/vertex-ai-484314/vertex-ai-pipeline-thang/training:latest
pipelineInfo:
  name: evaluation
root:
  dag:
    outputs:
      artifacts:
        evaluation-metrics:
          artifactSelectors:
          - outputArtifactKey: metrics
            producerSubtask: evaluation
        html:
          artifactSelectors:
          - outputArtifactKey: html
            producerSubtask: evaluation
        metrics:
          artifactSelectors:
          - outputArtifactKey: metrics
            producerSubtask: evaluation
    tasks:
      evaluation:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-evaluation
        inputs:
          artifacts:
            model:
              componentInputArtifact: model
            preprocessed_dataset:
              componentInputArtifact: preprocessed_dataset
        taskInfo:
          name: evaluation
  inputDefinitions:
    artifacts:
      model:
        artifactType:
          schemaTitle: system.Model
          schemaVersion: 0.0.1
      preprocessed_dataset:
        artifactType:
          schemaTitle: system.Dataset
          schemaVersion: 0.0.1
  outputDefinitions:
    artifacts:
      evaluation-metrics:
        artifactType:
          schemaTitle: system.Metrics
          schemaVersion: 0.0.1
      html:
        artifactType:
          schemaTitle: system.HTML
          schemaVersion: 0.0.1
      metrics:
        artifactType:
          schemaTitle: system.Metrics
          schemaVersion: 0.0.1
schemaVersion: 2.1.0
sdkVersion: kfp-2.7.0
